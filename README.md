# Bi_F_LSTM
Partial re-implementation of Image Captioning with Deep Bidirectional LSTMs

#### Note: this is a work in progress. I will upload results with other datasets and a detailed explanation soon.

### Introduction:
This model is similar to Bi-S-LSTM model proposed in _Image Captioning with Deep Bidirectional LSTMs_ published in _24th ACM international conference on Multimedia_ [[link]](https://dl.acm.org/doi/abs/10.1145/2964284.2964299) . An updated paper was published in journal ACM TOMM [[link]](https://dl.acm.org/doi/abs/10.1145/3115432).
There are following differences in our implementation:
1. I have not used Data Augmentation in this implementation. However, I have included options for horizontal and vertical data augmentation in the code which can be used by setting use_data_augmentation = True in train.py.
2. I have used batch size of 32 for all experiments and learning rate of 0.0001.
3. I have used VGG-16 CNN for image feature extraction whereas the authors used both AlexNet and VGG-16 for experiments.
4. Since both forward and backward LSTMs are trained for caption generation, I have experimented with both the inference strategy used in the paper (where the most likely sentence generated by forward or backward LSTMs is used as caption) and separate inference with backward and forward LSTMs.
5. I could not find hidden and cell state initialization in the paper. So, I have initilized hidden and cell states as zero vectors for both forward and backward LSTMs.

### Method
I have used composite Bi-Directional LSTM framework with residual connections as described in paper. The first layer is Text-LSTM (T-LSTM) and the final layer is Multimodal-LSTM (M-LSTM). The first T-LSTM layer takes as input, the word vector representations. An intermediate Fully Connected (FC) Layer is takes as input the output hidden state of T-LSTM layer. The output of FC Layer and hidden state of T-LSTM are combined and provided as input to M-LSTM layer along with Image Feature representation. Since, it is not clear from the paper as to how T-LSTM hidden state, FC output and image representation are combined, I have concatenated all components together and used them as input to Multimodal-LSTM (M-LSTM). I have evaluated with 1, 3, 5, 10, 15 and 20 as beam sizes. In the paper, authors have used greedy evaluation with beam size as 1.

In the paper, both forward and backward LSTMs are trained to generate captions and their losses are combined. During evaluation the captions generated by forward and backward LSTM are evaluated and most likely caption is selected at each time-step. In this implementation, I have saved captions generated by both forward and backward LSTMs separately and also recorded caption generated by overall model (i.e., the most likely caption, from backward and forward LSTMs, recorded at each time-step.) 


### Reproducing the results:
1. Download 'Karpathy Splits' for train, validation and testing from [here](http://cs.stanford.edu/people/karpathy/deepimagesent/caption_datasets.zip).
2. For evaluation, the model already generates BLEU scores. In addition, it saves results and image annotations as needed in MSCOCO evaluation format. So for generation of METEOR, CIDEr, ROUGE-L and SPICE evaluation metrics, the evaluation code can be downloaded from [here](https://github.com/cocodataset/cocoapi/tree/master/PythonAPI).

#### Prerequisites:
1. This code has been tested on python 3.6.9 but should word on all python versions > 3.6.
2. Pytorch v1.5.0
3. CUDA v10.1
4. Torchvision v0.6.0
5. Numpy v.1.15.0
6. pretrainedmodels v0.7.4 (Install from [source](https://github.com/Cadene/pretrained-models.pytorch.git)). (I think all versions will work but I have listed here for the sake of completeness.)


#### Execution:
1. First set the path to Flickr8k/Flickr30k/MSCOCO data folders in create_input_files_dataname.py file ('dataname' replaced by f8k/f30k/coco).
2. Create processed dataset by running: 
> python create_input_files_dataname.py

3. To train the model:
> python train_dataname.py

4. To evaluate: 
> python eval_dataname.py beamsize 

(eg.: python train_f8k.py 20)
